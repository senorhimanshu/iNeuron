{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is an unsupervised learning approach? Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.\n",
    "\n",
    "The number one advantage of unsupervised learning is the ability for a machine to tackle problems that humans might find insurmountable either due to a limited capacity or a bias. Unsupervised learning is ideal for exploring raw and unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised approach which finds a structure/pattern in a collection of unlabeled data. A cluster is a collection of objects which are ‚Äúsimilar‚Äù amongst themselves and are ‚Äúdissimilar‚Äù to the objects belonging to a different cluster. For example:\n",
    "\n",
    "![](clustering.png)\n",
    "\n",
    "In the figure above, we can easily identify 4 different clusters. The clustering criteria here is distance. Whichever points are near to each other are kept in the same cluster and the faraway points belong to a different cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tHow do clustering and classification differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels. Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat are the various applications of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For customer segmentation: You can cluster your customers based on their purchases,their activity on your website, and so on. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, this can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.\n",
    "\n",
    "- For data analysis: When analyzing a new dataset, it is often useful to first discover clusters of similar instances, as it is often easier to analyze clusters separately.\n",
    "\n",
    "- As a dimensionality reduction technique: Once a dataset has been clustered, it is usually possible to measure each instance‚Äôs affinity with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance‚Äôs feature vector x can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k dimensional. This is typically much lower dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
    "\n",
    "- For anomaly detection (also called outlier detection): Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second, and so on. Anomaly detection is particularly useful in detecting defects in manufacturing, or for fraud detection.\n",
    "\n",
    "- For semi-supervised learning: If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This can greatly increase the amount of labels available for a subsequent supervised learning algorithm, and thus improve its performance. .\n",
    "\n",
    "- For search engines: For example, some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database: similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is to find this image‚Äôs cluster using the trained clustering model, and you can then simply return all the images from this cluster.\n",
    "\n",
    "- To segment an image: By clustering pixels according to their color, then replacing each pixel‚Äôs color with the mean color of its cluster, it is possible to reduce the number of different colors in the image considerably. This technique is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow does clustering play a role in supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve accuracy of supervised machine learning algorithm by using clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are the requirements to be met by a clustering algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary requirements that should be met by a clustering algorithm are:\n",
    "\n",
    "- It should be scalable\n",
    "- It should be able to deal with attributes of different types;\n",
    "- It should be able to discover arbitrary shape clusters;\n",
    "- It should have an inbuilt ability to deal with noise and outliers;\n",
    "- The clusters should not vary with the order of input records;\n",
    "- It should be able to handle data of high dimensions.\n",
    "- It should be easy to interpret and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tDiscuss the different approaches for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering approaches can be broadly divided into two categories: Agglomerative and Divisive.\n",
    "\n",
    "- Agglomerative: This approach first considers all the points as individual clusters and then finds out the similarity between two points, puts them into a cluster. Then it goes on finding similar points and clusters until there is only one cluster left i.e., all points belong to a big cluster. This is also called the bottom-up approach.\n",
    "\n",
    "- Divisive: It is opposite of the agglomerative approach. It first considers all the points to be part of one big cluster and in the subsequent steps tries to find out the points/ clusters which are least similar to each other and then breaks the bigger cluster into smaller ones. This continues until there are as many clusters as there are datapoints. This is also called the top-down approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is WCSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within Cluster Sum of Squares\n",
    "One measurement is Within Cluster Sum of Squares (WCSS), which measures the squared average distance of all the points within a cluster to the cluster centroid. To calculate WCSS, you first find the Euclidean distance (see figure below) between a given point and the centroid to which it is assigned. You then iterate this process for all points in the cluster, and then sum the values for the cluster and divide by the number of points. Finally, you calculate the average across all clusters. This will give you the average WCSS.\n",
    "\n",
    "$$WCSS= \\sum \\sum (x-m_i)^2$$\n",
    "\n",
    "![](euclidean-distance.png)\n",
    "\n",
    "![](wcss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tDiscuss the elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Elbow-Method:\n",
    "\n",
    "This method is based on the relationship between the within-cluster sum of squared distances(WCSS Or Inertia) and the number of clusters. It is observed that first with an increase in the number of clusters WCSS decreases steeply and then after a certain number of clusters the drop in WCSS is not that prominent. The point after which the graph between WCSS and the number of clusters becomes comparatively smother is termed as the elbow and the number of cluster at that point are the optimum number of clusters as even after increasing the clusters after that point the variation is not decreasing by much i.e., we have accounted for almost all the dissimilarity in the data. An elbow-curve looks like:\n",
    "\n",
    "![](elbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhat is the significance of ‚ÄòK‚Äô in K-Means and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K refers to the number of clusters.The value of K needs to be specified first and then the algorithm assigns the points to exactly one cluster.\n",
    "\n",
    "The theory discussed above can be mathematically expressed as:\n",
    "\n",
    "- Let C1, C2, Ck be the K clusters\n",
    "\n",
    "- Then we can write: ùê∂1ùëàùê∂2ùëàùê∂3ùëà‚Ä¶ùëàùê∂ùëò={1,2,3,‚Ä¶,n} i.e., each datapoint has been assigned to a cluster.\n",
    "\n",
    "- Also,\n",
    "\n",
    "![](non_overlapping.png)\n",
    "\n",
    "This means that the clusters are non-overlapping.\n",
    "\n",
    "- The idea behind the K-Means clustering approach is that the within-cluster variation amongst the point should be minimum. The within-cluster variance is denoted by: W(Ck).Hence, according to the statement above, we need to minimize this variance for all the clusters. Mathematically it can be written as:\n",
    "\n",
    "![](minimize_ck.png)\n",
    "\n",
    "- The next step is to define the criterion for measuring the within-cluster variance. Generally, the criterion is the Euclidean distance between two data points.\n",
    "\n",
    "![](wck_formula.png)\n",
    "\n",
    "- The above formula says that we are calculating the distances between all the point in a cluster, then we are repeating it for all the K clusters(That‚Äôs why two summation signs) and then we are dividing it by the number of observation in the clusters (Ck is the number of observations in the Kth cluster) to calculate the average.\n",
    "\n",
    "So, ultimately our goal is to minimize:\n",
    "\n",
    "![](final_ck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tDiscuss the step by step implementation of K-Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly assign K centres.\n",
    "2. Calculate the distance of all the points from all the K centres and allocate the points to cluster based on the shortest distance. The model‚Äôs inertia is the mean squared distance between each instance and its closest centroid. The goal is to have a model with the lowes intertia.\n",
    "3. Once all the points are assigned to clusters, recompute the centroids.\n",
    "4. Repeat the steps 2 and 3 until the locations of the centroids stop changing and the cluster allocation of the points becomes constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat are the challenges with K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We need to specify the number of clusters beforehand.\n",
    "2. It is required to run the algorithm multiple times to avoid a sub-optimal solution\n",
    "3. K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the various improvements in K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The clusters sometimes vary based on the initial choice of the centroids. An important improvement to the K-Means algorithm, called K-Means++, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii. They introduced a smarter initialization step that tends to select centroids that are distant from one another, and this makes the K-Means algorithm much less likely to converge to a suboptimal solution.\n",
    "2. Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan.It considerably accelerates the algorithm by avoiding many unnecessary distance calculations: this is achieved by exploiting the triangle inequality (i.e., the straight line is always the shortest; in a triangle with sides a,b and c=> a+b>c) and by keeping track of lower and upper bounds for distances between instances and centroids.\n",
    "3. Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley. Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. You can just use this class like the KMeans class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering:\n",
    "14.\tDiscuss the agglomerative and divisive clustering approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering approaches can be broadly divided into two categories: Agglomerative and Divisive.\n",
    "\n",
    "Agglomerative: This approach first considers all the points as individual clusters and then finds out the similarity between two points, puts them into a cluster. Then it goes on finding similar points and clusters until there is only one cluster left i.e., all points belong to a big cluster. This is also called the bottom-up approach.\n",
    "\n",
    "Divisive: It is opposite of the agglomerative approach. It first considers all the points to be part of one big cluster and in the subsequent steps tries to find out the points/ clusters which are least similar to each other and then breaks the bigger cluster into smaller ones. This continues until there are as many clusters as there are datapoints. This is also called the top-down approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhat are dendrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters. The dendrogram below shows the hierarchical clustering of six observations shown on the scatterplot to the left.\n",
    "\n",
    "![](dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tDiscuss the Hierarchical clustering in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Algorithm\n",
    "\n",
    "Also called Hierarchical cluster analysis or HCA is an unsupervised clustering algorithm which involves creating clusters that have predominant ordering from top to bottom.\n",
    "\n",
    "For e.g: All files and folders on our hard disk are organized in a hierarchy.\n",
    "\n",
    "The algorithm groups similar objects into groups called clusters. The endpoint is a set of clusters or groups, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\n",
    "\n",
    "This clustering technique is divided into two types:\n",
    "\n",
    "1. Agglomerative Hierarchical Clustering\n",
    "2. Divisive Hierarchical Clustering\n",
    "\n",
    "#### Agglomerative Hierarchical Clustering\n",
    " \n",
    "The Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It‚Äôs also known as AGNES (Agglomerative Nesting). It's a ‚Äúbottom-up‚Äù approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "1. Make each data point a single-point cluster ‚Üí forms N clusters\n",
    "2. Take the two closest data points and make them one cluster ‚Üí forms N-1 clusters\n",
    "3. Take the two closest clusters and make them one cluster ‚Üí Forms N-2 clusters.\n",
    "4. Repeat step-3 until you are left with only one cluster.\n",
    "\n",
    "Have a look at the visual representation of Agglomerative Hierarchical Clustering for better understanding:\n",
    "\n",
    "<img src=\"Agglomerative Hierarchical Clustering.gif\" width=\"400\" align=\"center\">\n",
    "\n",
    "There are several ways to measure the distance between clusters in order to decide the rules for clustering, and they are often called Linkage Methods. Some of the common linkage methods are:\n",
    "\n",
    "- Complete-linkage: the distance between two clusters is defined as the longest distance between two points in each cluster.\n",
    "\n",
    "![](complete_linkage.png)\n",
    "\n",
    "- Single-linkage: the distance between two clusters is defined as the shortest distance between two points in each cluster. This linkage may be used to detect high values in your dataset which may be outliers as they will be merged at the end.\n",
    "\n",
    "![](single_linkage.png)\n",
    "\n",
    "- Average-linkage: the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster.\n",
    "\n",
    "![](average_linkage.png)\n",
    "\n",
    "- Centroid-linkage: finds the centroid of cluster 1 and centroid of cluster 2, and then calculates the distance between the two before merging.\n",
    "\n",
    "The choice of linkage method entirely depends on you and there is no hard and fast method that will always give you good results. Different linkage methods lead to different clusters.\n",
    "\n",
    "The point of doing all this is to demonstrate the way hierarchical clustering works, it maintains a memory of how we went through this process and that memory is stored in Dendrogram.\n",
    "\n",
    "#### What is a Dendrogram?\n",
    "\n",
    "A Dendrogram is a type of tree diagram showing hierarchical relationships between different sets of data.\n",
    "\n",
    "As already said a Dendrogram contains the memory of hierarchical clustering algorithm, so just by looking at the Dendrgram you can tell how the cluster is formed.\n",
    "\n",
    "<img src=\"dendrogram.gif\" width=\"600\" align=\"center\">\n",
    "\n",
    "Note:- \n",
    "\n",
    "1. Distance between data points represents dissimilarities.\n",
    "2. Height of the blocks represents the distance between clusters.\n",
    "\n",
    "So you can observe from the above figure that initially P5 and P6 which are closest to each other by any other point are combined into one cluster followed by P4 getting merged into the same cluster(C2). Then P1and P2 gets combined into one cluster followed by P0 getting merged into the same cluster(C4). Now P3 gets merged in cluster C2 and finally, both clusters get merged into one.\n",
    "\n",
    "Parts of a Dendrogram\n",
    "\n",
    "![](dendrogram_parts.png)\n",
    "\n",
    "A dendrogram can be a column graph (as in the image below) or a row graph. Some dendrograms are circular or have a fluid-shape, but the software will usually produce a row or column graph. No matter what the shape, the basic graph comprises the same parts:\n",
    "\n",
    "- The Clades are the branch and are arranged according to how similar (or dissimilar) they are. Clades that are close to the same height are similar to each other; clades with different heights are dissimilar ‚Äî the greater the difference in height, the more dissimilarity.\n",
    "- Each clade has one or more leaves.\n",
    "- Leaves A, B, and C are more similar to each other than they are to leaves D, E, or F.\n",
    "- Leaves D and E are more similar to each other than they are to leaves A, B, C, or F.\n",
    "- Leaf F is substantially different from all of the other leaves.\n",
    "\n",
    "A clade can theoretically have an infinite amount of leaves. However, the more leaves you have, the harder the graph will be to read with the naked eye.\n",
    "\n",
    "One question that might have intrigued you by now is how do you decide when to stop merging the clusters?\n",
    "\n",
    "You cut the dendrogram tree with a horizontal line at a height where the line can traverse the maximum distance up and down without intersecting the merging point.\n",
    "\n",
    "For example in the below figure L3 can traverse maximum distance up and down without intersecting the merging points. So we draw a horizontal line and the number of verticle lines it intersects is the optimal number of clusters.\n",
    "\n",
    "![](dendrogram_reading.png)\n",
    "\n",
    "Number of Clusters in this case = 3.\n",
    "\n",
    "#### Divisive Hierarchical Clustering\n",
    " \n",
    "In Divisive or DIANA(DIvisive ANAlysis Clustering) is a top-down clustering method where we assign all of the observations to a single cluster and then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each observation. So this clustering approach is exactly opposite to Agglomerative clustering.\n",
    "\n",
    "![](divisive_hierarchial_clustering.png)\n",
    "\n",
    "There is evidence that divisive algorithms produce more accurate hierarchies than agglomerative algorithms in some circumstances but is conceptually more complex.\n",
    "\n",
    "In both agglomerative and divisive hierarchical clustering, users need to specify the desired number of clusters as a termination condition(when to stop merging).\n",
    "\n",
    "#### Measuring the goodness of Clusters\n",
    " \n",
    "Well, there are many measures to do this, perhaps the most popular one is the Dunn's Index. Dunn's index is the ratio between the minimum inter-cluster distances to the maximum intra-cluster diameter. The diameter of a cluster is the distance between its two furthermost points. In order to have well separated and compact clusters you should aim for a higher Dunn's index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tDiscuss the various linkage methods for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, hierarchical clustering starts out with clusters consisting of individual points.\n",
    "\n",
    "Later, it compares clusters with each other and merges the two \"closest\" clusters.\n",
    "\n",
    "Since clusters are sets of points, there are many different kinds of linkage methods:\n",
    "\n",
    "- Single Linkage: cluster distance = smallest pairwise distance\n",
    "- Complete Linkage: cluster distance = largest pairwise distance\n",
    "- Average Linkage: cluster distance = average pairwise distance\n",
    "- Centroid Linkage: cluster distance= distance between the centroids of the clusters\n",
    "- Ward‚Äôs Linkage: cluster criteria= Minimize the variance in the cluster\n",
    "\n",
    "#### Single Linkage: \n",
    "\n",
    "Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.\n",
    "\n",
    "cluster distance is the smallest distance between any point in cluster 1 and any point in cluster 2\n",
    "highly sensitive to outliers when forming flat clusters\n",
    "works well for low-noise data with an unusual structure\n",
    "\n",
    "#### Complete Linkage: \n",
    "\n",
    "Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.\n",
    "\n",
    "cluster distance is the largest distance between any point in cluster 1 and any point in cluster 2\n",
    "less sensitive to outliers than single linkage\n",
    "\n",
    "#### Average Linkage: \n",
    "\n",
    "Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n",
    "\n",
    "cluster distance is the average distance of all pairs of points in clusters 1 and 2\n",
    "\n",
    "#### Centroid Linkage: \n",
    "\n",
    "The dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.\n",
    "\n",
    "cluster distance is the distance of the centroids of both clusters\n",
    "\n",
    "#### Ward linkage: \n",
    "\n",
    "Wikipidea says Ward's minimum variance criterion minimizes the total within-cluster variance. To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging.\n",
    "\n",
    "- based on minimizing a variance criterion before and after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tDiscuss the differences between K-Means and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means Clustering\tvs Hierarchical Clustering\n",
    "1. k-means, using a pre-specified  number of clusters, the method  assigns records to each cluster to  find the mutually exclusive cluster  of spherical shape based on distance.\t \n",
    "\n",
    "    - Hierarchical methods can be  either divisive or agglomerative.\n",
    "\n",
    "2. K Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.\t\n",
    "    - In hierarchical clustering  one can stop at any number of clusters, one find appropriate by interpreting  the dendrogram.\n",
    "\n",
    "3. One can use median or mean as a cluster centre to represent each cluster.\t\n",
    "    - Agglomerative methods  begin with ‚Äòn‚Äô clusters and  sequentially combine similar clusters until only one cluster is obtained.\n",
    "\n",
    "4. Methods used are normally less computationally intensive and are suited with very large datasets.\t\n",
    "    - Divisive methods work in the opposite direction, beginning with one cluster that includes all the records and Hierarchical methods are  especially useful when the target is to arrange the clusters  into a natural hierarchy.\n",
    "\n",
    "5. In K Means clustering, since one  start with random choice of  clusters, the results produced by running the algorithm many times may differ.\t\n",
    "    - In Hierarchical Clustering, results are reproducible in Hierarchical clustering\n",
    "\n",
    "6. K- means clustering a simply a division of the set of data  objects into non- overlapping subsets (clusters) such that each  data object is in exactly one subset).\t\n",
    "    - A hierarchical clustering is a set of nested clusters that are arranged as a tree.\n",
    "\n",
    "7. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D,  sphere in 3D).\t\n",
    "    - Hierarchical clustering don‚Äôt work  as well as, k means when the  shape of the clusters is hyper  spherical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "19.\tDiscuss the basic terms used in DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an unsupervised machine learning algorithm.This algorithm defines clusters as continuous regions of high density.\n",
    "\n",
    "- Epsilon: This is also called eps. This is the distance till which we look for the neighbouring points.\n",
    "- Min_points: The minimum number of points specified by the user.\n",
    "- Core Points: If the number of points inside the eps radius of a point is greater than or equal to the min_points then it‚Äôs called a core point.\n",
    "- Border Points: If the number of points inside the eps radius of a point is less than the min_points and it lies within the eps radius region of a core point, it‚Äôs called a border point.\n",
    "- Noise: A point which is neither a core nor a border point is a noise point.\n",
    "\n",
    "Let‚Äôs say if the eps=1 and min_points =4\n",
    "\n",
    "![](dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tDiscuss the step by step implementation of DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm Steps:\n",
    "\n",
    "1. The algorithm starts with a random point in the dataset which has not been visited yet and its neighbouring points are identified based on the eps value.\n",
    "2. If the point contains greater than or equal points than the min_pts, then the cluster formation starts and this point becomes a _core point_, else it‚Äôs considered as noise. The thing to note here is that a point initially classified as noise can later become a border point if it‚Äôs in the eps radius of a core point.\n",
    "3. If the point is a core point, then all its neighbours become a part of the cluster. If the points in the neighbourhood turn out to be core points then their neighbours are also part of the cluster.\n",
    "4. Repeat the steps above until all points are classified into different clusters or noises.\n",
    "\n",
    "This algorithm works well if all the clusters are dense enough, and they are well separated by low-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Evaluation\n",
    "21.\tWhat are the aspects of cluster validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspects of cluster validation:\n",
    "    \n",
    "1. External: Compare your cluster to the ground truth.\n",
    "2. Internal: Evaluating the cluster without reference to external data.\n",
    "3. Reliability: The clusters are not formed by chance(randomly)- some statistical framework can be used.\n",
    "\n",
    "Cluster Validity:\n",
    "\n",
    "The validation of clusters created is a troublesome task. The problem here is: ‚Äùclusters are in the eyes of the beholder‚Äù\n",
    "\n",
    "A good cluster will have:\n",
    "\n",
    "- High inter-class similarity, and\n",
    "- Low intraclass similarity\n",
    "\n",
    "![](inter-class-and-intra-class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is a confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix:**\n",
    "\n",
    "$n$= number of points\n",
    "\n",
    "$m_i$=points in _cluster i_\n",
    "\n",
    "$c_j$=points in _class j_\n",
    "\n",
    "$n_{ij}$=  points in cluster i coming from cluster j\n",
    "\n",
    "$p_{ij}=\\frac{n_{ij}}{m_i}$= probability of element from cluster i to be assigned to class j\n",
    "\n",
    "![](conf1_clustering.png)\n",
    "\n",
    "![](conf2_clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is Jaccard‚Äôs coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N: Number of objects in the data\n",
    "P: ${P_1,P_2,‚Ä¶,P_m}$ the set of ground truth clusters\n",
    "C: ${C_1,C_2,‚Ä¶C_n}$ the set of clusters formed by the algorithm\n",
    "\n",
    "The Incidence Matrix:\n",
    "N* N matrix\n",
    "\n",
    "$P_ij =1$ if the two points $O_i$ and $O_j$ belong to the same cluster in the ground truth else $P_ij=0$ \n",
    "\n",
    "$C_ij =1$ if the two points $O_i$ and $O_j$ belong to the same cluster in the ground truth else $C_ij=0$ \n",
    "\n",
    "Now there can be the following scenarios:\n",
    "1.\t$C_ij=P_ij=1$ --> both the points belong to the same cluster for both our algorithm and ground truth(Agree)--- **SS**\n",
    "2.\t$C_ij=P_ij=0$ --> both the points don‚Äôt belong to the same cluster for both our algorithm and ground truth(Agree)--- **DD**\n",
    "3.\t$_Cij=1 but P_ij=0$ --> The points belong in the same cluster for our algorithm but in different clusters for the ground truth (Disagree)---- **SD**\n",
    "4.\t$C_ij=0 but P_ij=1$ --> The points don‚Äôt belong in the same cluster for our algorithm but in same clusters for the ground truth (Disagree)----**DS**\n",
    "\n",
    "**Jaccard Coeeficient**=$\\frac{ SS}{(SS+SD+DS)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tWhat is Rand Index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rand Index**= $\\frac{Total Agree}{Total Disagree}=\\frac{(SS+DD)}{(SS+DD+DS+SD)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is the entropy of a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy** \n",
    "Entropy of Cluster i, given by $e_i= - \\sum p_{ij} log (p_{ij})$\n",
    "\n",
    "For the entire clustering algorithm, the entropy can be given as: \n",
    "$e= \\sum \\frac{m_i}{n}e_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tDiscuss the purity of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purity**\n",
    " The purity is the total percentage of data points clustered correctly.\n",
    "\n",
    " The purity of  cluster i, given by $p_i=max (p_{ij})$ \n",
    "\n",
    "And for the entire cluster it is: $p(C)=\\sum \\frac{m_i}{n}p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tWhat are cohesion and compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods use to measure the quality of clusters without external references. There are two aspects to it.\n",
    "\n",
    "* **Cohesion:** How closely the objects in the same cluster are related to each other. It is the within-cluster sum of squared distances. It is the same metric that we used to calculate for the K-Means algorithm.\n",
    "$WCSS= \\sum \\sum (x-m_i)^2$\n",
    "\n",
    "* **Separation:** How different the objects in different clusters are and how distinct a well-separated cluster is from other clusters. It is the between cluster sum of squared distances. $BSS=\\sum C_i(m-m_i)^2$\n",
    "\n",
    "Where C is the size of the individual cluster and m is the centroid of all the data points.\n",
    "\n",
    "**Note:** BSS+WSS is always a constant.\n",
    "\n",
    "The silhouette  can be calculated as:\n",
    "![](silhouette.png)\n",
    "\n",
    "Where a(x) is the avarage distance of x from all the other points in the same cluster and b(x) is the avarage distance of x from all the other points in the other clusters.\n",
    "\n",
    "\n",
    "And the Silhoeutte coefficient is given by:\n",
    "\n",
    "$SC=\\frac{1}{N} \\sum S(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tWhat are the steps for AWS deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhat difficulties did you face while deploying to AWS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
