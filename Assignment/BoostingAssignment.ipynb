{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers. The weights are assigned based on the performance of an individual tree.\n",
    "\n",
    "![](boosting.png)\n",
    "\n",
    "Ensemble parameters are calculated in stagewise way which means that while calculating the subsequent weight, the learning from the previous tree is considered as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences Between Bagging and Boosting ‚Äì\n",
    "\n",
    "S.NO\t- BAGGING\t- BOOSTING\n",
    "1.\t- Simplest way of combining predictions that\n",
    "belong to the same type.\t- A way of combining predictions that\n",
    "belong to the different types.\n",
    "2.\t- Aim to decrease variance, not bias.\t- Aim to decrease bias, not variance.\n",
    "3.\t- Each model receives equal weight.\t- Models are weighted according to their performance.\n",
    "4.\t- Each model is built independently.\t- New models are influenced\n",
    "by performance of previously built models.\n",
    "5.\t- Different training data subsets are randomly drawn with replacement from the entire training dataset.\t- Every new subsets contains the elements that were misclassified by previous models.\n",
    "6.\t- Bagging tries to solve over-fitting problem.\t- Boosting tries to reduce bias.\n",
    "7.\t- If the classifier is unstable (high variance), then apply bagging.\t- If the classifier is stable and simple (high bias) the apply boosting.\n",
    "8.\t- Random forest.\t- Gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image result for what are weak and strong classifier in adaboost\n",
    "Boosting algorithms are a set of the low accurate classifier to create a highly accurate classifier. Low accuracy classifier (or weak classifier) offers the accuracy better than the flipping of a coin. Highly accurate classifier( or strong classifier) offer error rate close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees provide an effective method of Decision Making because they: Clearly lay out the problem so that all options can be challenged. Allow us to analyze fully the possible consequences of a decision. Provide a framework to quantify the values of outcomes and the probabilities of achieving them.\n",
    "\n",
    "Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. ... The second tree is therefore grown on this weighted data. Here, the idea is to improve upon the predictions of the first tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Classifier:\n",
    "\n",
    "Ada-boost or Adaptive Boosting is one of ensemble boosting classifier proposed by Yoav Freund and Robert Schapire in 1996. It combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Adaboost should meet two conditions:\n",
    "\n",
    "1. The classifier should be trained interactively on various weighed training examples.\n",
    "2. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.\n",
    "\n",
    "How does the AdaBoost algorithm work?\n",
    "\n",
    "It works in the following steps:\n",
    "\n",
    "1. Initially, Adaboost selects a training subset randomly.\n",
    "2. It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n",
    "3. It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n",
    "4. Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n",
    "5. This process iterate until the complete training data fits without any error or until reached to the specified maximum number of estimators.\n",
    "6. To classify, perform a \"vote\" across all of the learning algorithms you built.\n",
    "\n",
    "![](adaboost_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial guess of the Gradient Boosting algorithm is to predict the average value of the target y . ... For the variable x1 , we compute the difference between the observations and the prediction we made. This is called the pseudo-residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees use decision trees as estimators. It can work with different loss functions (regression, classification, risk modelling etc.), evaluate it's gradient and approximates it with a simple tree (stage-wisely, that minimizes the overall error).\n",
    "\n",
    "AdaBoost is a special case of Gradient Boosted Tree that uses exponential loss function.\n",
    "\n",
    "The Algorithm:\n",
    "\n",
    "- Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "\n",
    "- Calculate the pseudo residuals.\n",
    "\n",
    " Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "derivative of the pseudo residual=$$(\\frac{ùõøùêø(ùë¶ùëñ,ùëì(ùë•ùëñ))} {ùõø(ùëì(ùë•ùëñ))})$$\n",
    "where, L is the loss function.\n",
    "\n",
    " Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "- create a tree to predict the pseudo residuals instead of a tree to predict for the actual column values.\n",
    "- new result= previous result+learning rate* residual\n",
    "\n",
    "Mathematically, ùêπ1(ùë•)=ùêπ0(ùë•)+ùúà‚àëùõæ\n",
    "where ùúà is the learning rate and ùõæ is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost improves the gradient boosting method even further.\n",
    "\n",
    "XGBoost (extreme gradient boosting) regularises data better than normal gradient boosted Trees.\n",
    "\n",
    "It was developed by Tianqi Chen in C++ but now has interfaces for Python, R, Julia.\n",
    "\n",
    "XGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors (ùëó trees). In the formula ùëìùëó means a prediction coming from the $j^th$ tree.\n",
    "\n",
    "$$obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{j=1}^{j} \\Omega (f_j)$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$$\n",
    "First part $(\\gamma T)$ is responsible for controlling the overall number of created leaves, and the second term $$(\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2)$$ watches over the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main advantages:\n",
    "\n",
    "- out of the box feature of appropriate bias-variance trade-off,\n",
    "- great computation speed as it utilises parallel computing and cache optimization,\n",
    "- uses hardware optimization,\n",
    "- works well even if the features are correlated\n",
    "- robust even if there is noise for classification problem\n",
    "- the facility of early stopping\n",
    "- the package is evolving, i.e., new features are being added.\n",
    "\n",
    "![](xgboost.png)\n",
    "\n",
    "System Optimization:\n",
    "\n",
    "1. Parallelization: XGBoost approaches the process of sequential tree building using parallelized implementation. This is possible due to the interchangeable nature of loops used for building base learners; the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation.\n",
    "\n",
    "2. Tree Pruning: The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses ‚Äòmax_depth‚Äô parameter as specified instead of criterion first, and starts pruning trees backward. This ‚Äòdepth-first‚Äô approach improves computational performance significantly.\n",
    "\n",
    "3. Hardware Optimization: This algorithm has been designed to make efficient use of hardware resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‚Äòout-of-core‚Äô computing optimize available disk space while handling big data-frames that do not fit into memory.\n",
    "\n",
    "Algorithmic Enhancements:\n",
    "\n",
    "1. Regularization: It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n",
    "2. Sparsity Awareness: XGBoost naturally admits sparse features for inputs by automatically ‚Äòlearning‚Äô best missing value depending on training loss and handles different types of sparsity patterns in the data more efficiently.\n",
    "3. Weighted Quantile Sketch: XGBoost employs the distributed weighted Quantile Sketch algorithm to effectively find the optimal split points among weighted datasets.\n",
    "4. Cross-validation: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\n",
    "\n",
    "### Evolution of XGBoost Algorithm from Decision Trees:\n",
    "\n",
    "![](evolution-of-algorithms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
