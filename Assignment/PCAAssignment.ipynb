{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExplain the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the phenomena that occur when classifying, organizing, and analyzing high dimensional data that does not occur in low dimensional spaces, specifically the issue of data sparsity and “closeness” of data.\n",
    "\n",
    "Humans are bound by their perception of a maximum of three dimensions. We can’t comprehend shapes/graphs beyond three dimensions. Often, data scientists get datasets which have thousands of features. They give birth to two kinds of problems:\n",
    "\n",
    "- Increase in computation time: Majority of the machine learning algorithms they rely on the calculation of distance for model building and as the number of dimensions increases it becomes more and more computation-intensive to create a model out of it.\n",
    "\n",
    "- Hard (or almost impossible) to visualise the relationship between features: As stated above, humans can not comprehend things beyond three dimensions. So, if we have an n-dimensional dataset, the only solution left to us is to create either a 2-D or 3-D graph out of it. Let’s say for simplicity, we are creating 2-D graphs. Suppose we have 1000 features in the dataset. That results in a total (1000*999)/2= 499500 combinations possible for creating the 2-D graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is a dimensionality reduction technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a feature selection technique using which we reduce the number of features to be used for making a model without losing a significant amount of information compared to the original dataset. In other words, a dimensionality reduction technique projects a data of higher dimension to a lower-dimensional subspace.\n",
    "\n",
    "When to use Dimensionality Reduction? \n",
    "\n",
    "Dimensionality reduction shall be used before feeding the data to a machine learning algorithm to achieve the following:\n",
    "\n",
    "- It reduces the size of the space in which the distances are calculated, thereby improving machine learning algorithm performance.\n",
    "- It reduces the degrees of freedom for our dataset avoiding chances of overfitting\n",
    "- Reducing the dimensionality using dimensionality reduction techniques can simplify the dataset facilitating a better description, visualisation, and insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tExplain PCA. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n",
    "\n",
    "What are the principal components? Principal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features.\n",
    "\n",
    "Let's take the following example where the data is distributed like the diagram on the left:\n",
    "![](PCA1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is explained variance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It represents the amount of variance each principal component is able to explain.\n",
    "\n",
    "For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n",
    "\n",
    "- EVR of PC1=𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶1 𝑝𝑜𝑖𝑛𝑡𝑠 / (𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶1𝑝𝑜𝑖𝑛𝑡𝑠+𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶2 𝑝𝑜𝑖𝑛𝑡𝑠)=50/55=0.91\n",
    "- EVR of PC2=𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶2𝑝𝑜𝑖𝑛𝑡𝑠 / (𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶1 𝑝𝑜𝑖𝑛𝑡𝑠+𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑜𝑓 𝑃𝐶2 𝑝𝑜𝑖𝑛𝑡𝑠)=55/5=0.09\n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n",
    "\n",
    "The fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance. For several principal components, add up their variances and divide by the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is a scree plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree plots are the graphs that convey how much variance is explained by corresponding Principal components.\n",
    "\n",
    "![](scree.png)\n",
    "\n",
    "As shown in the given diagram, around 75 principal components explain approximately 90 % of the variance. Hence, 75 can be a good choice based on the scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tHow is the optimum number of principal components obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scree plot, dimensions corresponding to approx. 90% EVR can be considered as optimum number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat is covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is the measure of how a variable changes or varies and co means together. Hence, covariance is the measure of how two variables change together.\n",
    "\n",
    "![](covariance.png)\n",
    "\n",
    "If the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too. Generally, we avoid using highly correlated variables in building a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is the transpose of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. We denote the transpose of matrix A by AT.\n",
    "\n",
    "Example- Find the transpose of the given matrix\n",
    "\n",
    "M = \\begin{bmatrix} 2 & -9 & 3 \\\\ 13 & 11 & -17 \\\\ 3 & 6 & 15 \\\\ 4 & 13 & 1 \\end{bmatrix}\n",
    "\n",
    "Solution- Given a matrix of the order 4×3.\n",
    "\n",
    "Transpose of a matrix is given by interchanging of rows and columns.\n",
    "\n",
    "M^T = \\begin{bmatrix} 2 & 13 & 3 & 4 \\\\ -9 & 11 & 6 & 13\\\\ 3 & -17 & 15 & 1 \\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is an Eigen value and Eigen Vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue, often denoted by $\\lambda$, is the factor by which the eigenvector is scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy the Principal Components are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the second Principal Component should capture the highest variance from what is left after the first Principal Component explains the data as much as it can. (The first principal component has the largest possible variance, that is, accounts for as much of the variability in the data as possible.)\n",
    "\n",
    "![](pca_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tExplain the Eigen Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are unit vectors with length or magnitude equal to 1. They are often referred to as right vectors, which simply means a column vector.\n",
    "\n",
    "Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude.\n",
    "So, PCA is a method that:\n",
    "- Measures how each variable is associated with one another using a Covariance matrix\n",
    "- Understands the directions of the spread of our data using Eigenvectors\n",
    "- Brings out the relative importance of these directions using Eigenvalues\n",
    "\n",
    "An eigenvalue is a number, telling you how much variance there is in the data in that direction, in the example above the eigenvalue is a number telling us how spread out the data is on the line. The eigenvector with the highest eigenvalue is therefore the principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow can PCA be used for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the data compression by reducing the number of eigenvectors used to reconstruct the original data matrix. In that way the amount of required storage space is reduced.\n",
    "\n",
    "Eigen-decomposition Approach to PCA data compression:\n",
    "\n",
    "1. Normalize columns of 𝐴 so that each feature has a mean of zero\n",
    "2. Compute sample covariance matrix Σ=𝐴𝑇𝐴/(𝑚−1)\n",
    "3. Perform eigen-decomposition of Σ using np.linalg.eig(Sigma)\n",
    "4. Compress by ordering 𝑘 evectors according to largest e-values and compute 𝐴𝑋𝑘\n",
    "5. Reconstruct from the compressed version by computing 𝐴𝑋𝑘𝑋𝑇𝑘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the pros and cons of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of PCA:\n",
    "\n",
    "- Correlated features are removed.\n",
    "- Model training time is reduced.\n",
    "- Overfitting is reduced.\n",
    "- Helps in better visualizations\n",
    "- Ability to handle noise\n",
    "\n",
    "Cons of PCA\n",
    "\n",
    "- The resultant principal components are less interpretable than the original data\n",
    "- Can lead to information loss if the explained variance threshold is not considered appropriately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
