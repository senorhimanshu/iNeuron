{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextAnalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat is the role of NLP in text analysis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is a part of Artificial Intelligence, developed for the machine to understand human language. The ultimate goal of NLP is to read, understand and make valuable conclusion of human language. It is a very tough job to do as human language has a lot of variation in terms of language, pronunciation etc. Although, in recent times there has been a major breakthrough in the field of NLP.\n",
    "\n",
    "Siri and Alexa are one such example of uses of NLP.\n",
    "\n",
    "We will use NLP for text analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tName some libraries used for NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Natural Languange Tool Kit (NLTK)\n",
    "* Spacy\n",
    "* TextBlob\n",
    "* Stanford NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat is Tokenization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization.\n",
    "Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization.\n",
    "\n",
    "Let's understand this with an example. Below is a given paragraph, let's see how tokenization works on it:\n",
    "\n",
    "\"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "\n",
    "* Sentence Tokenize:\n",
    "\n",
    "    ['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
    "    \n",
    "  'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
    "  \n",
    "  'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land      borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
    "  \n",
    "  'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']\n",
    "\n",
    "\n",
    "* Word tokenize:\n",
    "\n",
    "['India', '(', 'Hindi', ':', 'Bhārat', ')', ',', 'officially', 'the', 'Republic', 'of', 'India', ',', 'is', 'a', 'country', 'in', 'South',\n",
    " 'Asia', '.', 'It', 'is', 'the', 'seventh-largest', 'country', 'by', 'area', ',', 'the', 'second-most', 'populous', 'country', ',', 'and',\n",
    " 'the', 'most', 'populous', 'democracy', 'in', 'the', 'world', '.', 'Bounded', 'by', 'the', 'Indian',\n",
    " 'Ocean',\n",
    " 'on',\n",
    " 'the',\n",
    " 'south',\n",
    " ',',\n",
    " 'the',\n",
    " 'Arabian',\n",
    " 'Sea',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southwest',\n",
    " ',',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Bay',\n",
    " 'of',\n",
    " 'Bengal',\n",
    " 'on',\n",
    " 'the',\n",
    " 'southeast',\n",
    " ',',\n",
    " 'it',\n",
    " 'shares',\n",
    " 'land',\n",
    " 'borders',\n",
    " 'with',\n",
    " 'Pakistan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'west',\n",
    " ';',\n",
    " 'China',\n",
    " ',',\n",
    " 'Nepal',\n",
    " ',',\n",
    " 'and',\n",
    " 'Bhutan',\n",
    " 'to',\n",
    " 'the',\n",
    " 'north',\n",
    " ';',\n",
    " 'and',\n",
    " 'Bangladesh',\n",
    " 'and',\n",
    " 'Myanmar',\n",
    " 'to',\n",
    " 'the',\n",
    " 'east',\n",
    " '.',\n",
    " 'In',\n",
    " 'the',\n",
    " 'Indian',\n",
    " 'Ocean',\n",
    " ',',\n",
    " 'India',\n",
    " 'is',\n",
    " 'in',\n",
    " 'the',\n",
    " 'vicinity',\n",
    " 'of',\n",
    " 'Sri',\n",
    " 'Lanka',\n",
    " 'and',\n",
    " 'the',\n",
    " 'Maldives',\n",
    " ';',\n",
    " 'its',\n",
    " 'Andaman',\n",
    " 'and',\n",
    " 'Nicobar',\n",
    " 'Islands',\n",
    " 'share',\n",
    " 'a',\n",
    " 'maritime',\n",
    " 'border',\n",
    " 'with',\n",
    " 'Thailand',\n",
    " 'and',\n",
    " 'Indonesia',\n",
    " '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are different types of tokens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sentence token\n",
    "* word token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tWhat do you understand by POS tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parts Of Speech Tagging(POS tags)\n",
    "\n",
    "As the name suggests, it is a method of tagging individual words on the basis of it's parts of speech.\n",
    "\n",
    "Wikipedia definition : Part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "There are 9 parts of speech in grammars, but in NLP there are more than 9 POS tags based on different set of rules, such as:\n",
    "\n",
    "* NN noun, singular 'table'\n",
    "* NNS noun plural 'tables'\n",
    "* NNP proper noun, singular \n",
    "* NNPS proper noun, plural \n",
    "\n",
    "There are 4 types of division for noun only. Similarly, there are multiple divisions for other part of speeches.\n",
    "\n",
    "![](pos.png)\n",
    "\n",
    "![](pos_tags.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6)\tWhat is chunking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is a process of extracting phrases from unstructured text, which means analyzing a sentence to identify the constituents(Noun Groups, Verbs, verb groups, etc.) However, it does not specify their internal structure, nor their role in the main sentence. It works on top of POS tagging.\n",
    "\n",
    "![](chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is Stemming and Lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
    "\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "<img src=\"stem-vs-lemma.png\" style=\"width:500px;height:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tWhat is the difference between Stemming and Lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words, **stemming** technique only looks at the form of the word whereas **lemmatization** technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9)\tWhat is NER? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chunking, we read that we can set rules to keep different POS tags under one sinlge user defined tag. One such form of chunking in NLP is known as Named Entity Recognition.\n",
    "\n",
    "In NER, we try to group entities like people, places, countries, things etc. together.\n",
    "\n",
    "![](NER1.png)\n",
    "\n",
    "![](NER2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10)\tExplain parsing in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing** in NLP is the process of determining the syntactic structure of a text by analyzing its constituent words based on an underlying grammar (of the language).\n",
    "\n",
    "See this example grammar below, where each line indicates a rule of the grammar to be applied to an example sentence “Tom ate an apple”.\n",
    "\n",
    "**Example Grammar:**\n",
    "\n",
    "![](parsing_grammar.png)\n",
    "\n",
    "Then, the outcome of the parsing process would be a parse tree like the following, where sentence is the root, intermediate nodes such as noun_phrase, verb_phrase etc. have children - hence they are called non-terminals and finally, the leaves of the tree ‘Tom’, ‘ate’, ‘an’, ‘apple’ are called terminals.\n",
    "\n",
    "**Parse Tree:**\n",
    "\n",
    "![](parse_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "11)\tWhat do you understand by Word Vectorization? Name the different methods used for word vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization (Word Embedding)\n",
    "\n",
    "Word vectorization is the process of mapping words to a set of real numbers or vectors. This is done to process the given words using machine learning techniques and extract relevant information from them such that it can be used in further predicting words. Vectorization is done by comparing a given word to the corpus(collection) of the available words.\n",
    "There are many different methods used for vectorizing a given set of words. let's see some of the mosed popular ones:\n",
    "\n",
    "### Count Vectorizer\n",
    "\n",
    "Count vectorizer uses two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
    "\n",
    "#### Bag of Words Model\n",
    "BOW model is used in NLP to represent the given text/sentence/document as a collection (bag) of words without giving any importance to grammar or the occurrence order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "BOW1 = {I :2, went : 1, to : 1,have : 1, a : 1, cup: 1, of :1, coffee : 1, but :1, ended : 1, up :1,having : 1, with :1, her :1}\n",
    "\n",
    "BOW2 = {I : 1, don’t : 1, understand:1, what : 1 , is :1, the : 1, problem : 1, here : 1}\n",
    "\n",
    "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occurring terms. But the “stop words” are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn’t mean that the word is as important. To resolve this problem, “Tf-idf” was introduced. We will discuss about it later.\n",
    "\n",
    "#### n-gram model\n",
    "\n",
    "As discussed in bag of words model, BOW model doesn’t keep the sequence of words in a given text, only the frequency of words matters. It doesn’t take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text.n-gram model is used in such cases to keep the context of the given text intact. N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "      When, n= 1, we call it a “unigram”.\n",
    "\n",
    "             n=2, it is called a “bigram”. \n",
    "             \n",
    "             n=3, it is called a “trigram”.\n",
    "And so on.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "* Unigram \n",
    "\n",
    "[I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
    "\n",
    "* Bi-gram\n",
    "\n",
    "[I went], [went to],[to have],[have a],[a cup],[cup f],[of coffee],[coffee but],[but I],[I ended],[ended up],\n",
    "[up having],[having lunch],[lunch with],[with her]\n",
    "\n",
    "* Tri-gram\n",
    "\n",
    "[I went to], [went to have], [to have a], [have a cup],[ a cup of], [cup of coffee],[ of coffee but],[ coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
    "\n",
    "Note: We can clearly see that BOW model is nothing but n-gram model when n=1.\n",
    "\n",
    "* Skip-grams\n",
    "\n",
    "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. \n",
    "Example:\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
    "\n",
    "[I understand, don’t what, understand is, what the, is problem, the here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "12)\tDiscuss the working of Bag of words and “n-gram” model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model\n",
    "BOW model is used in NLP to represent the given text/sentence/document as a collection (bag) of words without giving any importance to grammar or the occurrence order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "BOW1 = {I :2, went : 1, to : 1,have : 1, a : 1, cup: 1, of :1, coffee : 1, but :1, ended : 1, up :1,having : 1, with :1, her :1}\n",
    "\n",
    "BOW2 = {I : 1, don’t : 1, understand:1, what : 1 , is :1, the : 1, problem : 1, here : 1}\n",
    "\n",
    "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occurring terms. But the “stop words” are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn’t mean that the word is as important. To resolve this problem, “Tf-idf” was introduced. We will discuss about it later.\n",
    "\n",
    "#### n-gram model\n",
    "\n",
    "As discussed in bag of words model, BOW model doesn’t keep the sequence of words in a given text, only the frequency of words matters. It doesn’t take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text.n-gram model is used in such cases to keep the context of the given text intact. N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "      When, n= 1, we call it a “unigram”.\n",
    "\n",
    "             n=2, it is called a “bigram”. \n",
    "             \n",
    "             n=3, it is called a “trigram”.\n",
    "And so on.\n",
    "\n",
    "Let’s understand this with an example:\n",
    "\n",
    "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
    "\n",
    "* Unigram \n",
    "\n",
    "[I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
    "\n",
    "* Bi-gram\n",
    "\n",
    "[I went], [went to],[to have],[have a],[a cup],[cup f],[of coffee],[coffee but],[but I],[I ended],[ended up],\n",
    "[up having],[having lunch],[lunch with],[with her]\n",
    "\n",
    "* Tri-gram\n",
    "\n",
    "[I went to], [went to have], [to have a], [have a cup],[ a cup of], [cup of coffee],[ of coffee but],[ coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
    "\n",
    "Note: We can clearly see that BOW model is nothing but n-gram model when n=1.\n",
    "\n",
    "* Skip-grams\n",
    "\n",
    "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. \n",
    "Example:\n",
    "\n",
    "Text2 = “I don’t understand, what is the problem here?”\n",
    "\n",
    "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
    "\n",
    "[I understand, don’t what, understand is, what the, is problem, the here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13)\tGive a detailed explanation of Tf-Idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf (Term frequency–Inverse document frequency)\n",
    "\n",
    "Wikipedia definition:  ” Tf-Idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The Tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.”\n",
    "\n",
    "\n",
    "### Term Frequency\n",
    "It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as:\n",
    "\n",
    "Term frequency = (Number of times a word appears in the document) / (Total number of words in the document)\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "Term frequency has a disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like ‘a’, ‘the’, ‘in’, ’of’ etc. appears more in the documents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less.\n",
    " To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurrence. Mathematically it is given as:\n",
    " \n",
    "Inverse Document Frequency = log [(Number of documents)/(Number of documents the word appears in)]   \n",
    "\n",
    "note: [log has base 2]\n",
    "\n",
    "\n",
    "*Tf-Idf Score = Term frequency * Inverse Document Frequency*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand more with an example:\n",
    "\n",
    "Doc 1: This is an example.\n",
    "\n",
    "Doc 2: We will see how it works.\n",
    "\n",
    "Doc 3: IDF can be confusing.\n",
    "\n",
    "\n",
    "\n",
    "<img src= \"tfidf.PNG\">\n",
    "\n",
    "In the above table, we have calculated the term frequency as well as inverse document frequency of each of the words present in the 3 documents given. \n",
    "\n",
    "Now, let's calculate the tf-idf score for each term. Since, words of one document is not present in another document, we will have tf-idf value 0 for them e.g. words of doc1 will have 0 tf-idf for doc2 and doc3.\n",
    "\n",
    "<img src= \"tfidf2.PNG\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
